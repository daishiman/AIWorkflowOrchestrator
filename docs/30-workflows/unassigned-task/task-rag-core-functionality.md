# RAGコア機能（ローカルナレッジ統合） - タスク指示書

## メタ情報

| 項目             | 内容                                |
| ---------------- | ----------------------------------- |
| タスクID         | TASK-RAG-CORE-001                   |
| タスク名         | RAGコア機能（ローカルナレッジ統合） |
| 分類             | 新規機能                            |
| 対象機能         | チャット・ワークスペース・AI連携    |
| 優先度           | 最高                                |
| 見積もり規模     | 大規模                              |
| ステータス       | 未実施                              |
| 発見元           | ユーザー要望                        |
| 発見日           | 2025-12-11                          |
| 発見エージェント | @product-manager                    |

---

## 1. なぜこのタスクが必要か（Why）

### 1.1 背景

本プロダクトの**目玉機能**として、ローカルに蓄積したナレッジ（ドキュメント、コード、メモ等）をRAG（Retrieval-Augmented Generation）技術で活用し、ユーザー固有のコンテキストに基づいた高品質な回答を生成する。これにより、汎用LLMでは得られない、ユーザーのプロジェクトや知識に特化した支援が可能になる。

### 1.2 問題点・課題

- LLMはユーザー固有の情報を知らない
- 毎回コンテキストを手動で提供する必要がある
- ローカルの知識が活用されていない
- プロジェクト固有の用語や慣習を理解できない

### 1.3 放置した場合の影響

- プロダクトの差別化ポイントがなくなる
- ユーザー体験が競合と同等に留まる
- ローカルナレッジの価値が活かせない
- AIアシストの品質が限定的

---

## 2. 何を達成するか（What）

### 2.1 目的

ワークスペースのファイルをベクトル化してインデックスし、チャット時に関連情報を自動的に取得・注入することで、ユーザー固有のコンテキストに基づいた高品質な回答を生成する。

### 2.2 最終ゴール

- ワークスペースファイルの自動インデックス化
- セマンティック検索による関連情報の取得
- チャット時の自動コンテキスト注入
- 取得ソースの表示（引用元の明示）
- インデックス管理UI

### 2.3 スコープ

#### 含むもの

**インデックス機能**

- ワークスペースファイルのスキャン
- テキスト抽出（Markdown, コード, PDF, テキスト）
- チャンク分割（セマンティックチャンキング）
- ベクトル化（埋め込み生成）
- ベクトルDBへの保存
- 差分インデックス（変更ファイルのみ更新）

**検索・取得機能**

- セマンティック検索
- ハイブリッド検索（キーワード + セマンティック）
- リランキング（関連度の高い順に並び替え）
- コンテキストウィンドウ管理

**チャット連携**

- クエリに基づく関連情報の自動取得
- コンテキストへの自動注入
- 取得ソースの表示（引用元ファイル・行番号）
- RAGの有効/無効切り替え

**管理機能**

- インデックス状態の表示
- 手動インデックス更新
- 除外パターン設定（.gitignore連携）
- インデックス対象フォルダの選択

#### 含まないもの

- クラウドベースのベクトルDB（ローカル優先）
- マルチモーダルRAG（画像内テキスト等）
- リアルタイムインデックス更新
- 外部ナレッジベースとの連携

### 2.4 成果物

- ファイルスキャン・テキスト抽出モジュール
- チャンク分割ロジック
- ベクトル化サービス
- ベクトルDB統合（SQLite-VSS / ChromaDB）
- 検索・取得API
- チャットへのコンテキスト注入ロジック
- インデックス管理UI
- 関連するユニットテスト

---

## 3. 技術アーキテクチャ（推奨）

### 3.1 ベクトルDB選択

**推奨: SQLite-VSS（ローカル優先）**

- ローカル完結
- 追加サーバー不要
- Electronアプリとの相性が良い

**代替: ChromaDB**

- 軽量で高機能
- Python統合が容易

### 3.2 埋め込みモデル選択

**推奨: OpenAI Embeddings (text-embedding-3-small)**

- 高品質
- 低コスト
- API経由で簡単

**代替: ローカル埋め込みモデル**

- Sentence-BERT
- オフライン動作可能

### 3.3 チャンク分割戦略

- 固定サイズチャンク（500トークン程度）
- セマンティックチャンク（段落・セクション単位）
- コードファイルは関数・クラス単位

---

## 4. 実行手順

### Phase構成

```
Phase 0: 要件定義（アーキテクチャ決定）
Phase 1: 設計
  - T-01-1: RAGアーキテクチャ設計
  - T-01-2: ベクトルDBスキーマ設計
  - T-01-3: 検索・取得API設計
  - T-01-4: UI設計
Phase 2: 設計レビューゲート
Phase 3: テスト作成（TDD: Red）
Phase 4: 実装（TDD: Green）
  - T-04-1: ファイルスキャン・テキスト抽出
  - T-04-2: チャンク分割
  - T-04-3: ベクトル化・インデックス
  - T-04-4: 検索・取得API
  - T-04-5: チャットへの注入
  - T-04-6: 管理UI
Phase 5: リファクタリング（TDD: Refactor）
Phase 6: 品質保証
Phase 7: 最終レビューゲート
Phase 8: 手動テスト検証
Phase 9: ドキュメント更新
```

### 主要な使用エージェント

- **@arch-police**: RAGアーキテクチャ設計
- **@db-architect**: ベクトルDBスキーマ設計
- **@logic-dev**: インデックス・検索ロジック
- **@gateway-dev**: 埋め込みAPI連携
- **@ui-designer**: 管理UI設計
- **@sec-auditor**: プライバシー・セキュリティ検証

---

## 5. 完了条件チェックリスト

### 機能要件

- [ ] ワークスペースファイルがインデックス化される
- [ ] セマンティック検索が動作する
- [ ] チャット時に関連情報が自動注入される
- [ ] 取得ソース（引用元）が表示される
- [ ] インデックス状態が確認できる
- [ ] 除外パターンが設定できる
- [ ] RAGの有効/無効が切り替えられる

### 品質要件

- [ ] 全テストがPASS
- [ ] 検索精度が基準値を満たす
- [ ] インデックス処理が許容時間内に完了
- [ ] メモリ使用量が許容範囲内

---

## 6. リスクと対策

| リスク                 | 影響度 | 発生確率 | 対策                             |
| ---------------------- | ------ | -------- | -------------------------------- |
| インデックス処理の遅延 | 中     | 高       | 非同期処理、差分インデックス     |
| 検索精度の低さ         | 高     | 中       | 適切なチャンク戦略、リランキング |
| ストレージ使用量の増大 | 中     | 中       | 圧縮、古いインデックスの削除     |
| 埋め込みAPIコスト      | 中     | 中       | ローカルモデルの選択肢提供       |

---

## 7. 備考

### レビュー指摘の原文

```
基本的にはラグこれの目玉の機能としては、ローカルに集めているナレッジの情報をラグ機能として、
自分のローカルの情報を最適な情報で、一番必要な情報を的確に取得してきて、
それを基に成果物を生成するというような形を取りたいと思っています。
```

### 関連タスク

- TASK-RAG-DATA-001: ワークスペースRAGデータ統合機能（要件相談版）
- TASK-WS-CHAT-EDIT-001: ワークスペースファイルのチャット編集機能
- TASK-CHAT-SYSPROMPT-001: システムプロンプト設定機能

### 補足事項

- **本プロダクトの目玉機能**として最優先で実装
- ローカルファースト設計（クラウド依存を最小化）
- プライバシーに配慮（データはローカルに保持）
- 将来的に外部ナレッジベースとの連携も視野に入れる
